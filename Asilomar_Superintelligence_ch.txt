00:00:09我来问你个问题，你只能用“是”，“不是”或者“不清楚”来回答。可以吗？ 
00:00:17那么，我们开始吧。 
00:00:21有没有可能出现某种形式的超级智能呢，Jaan? 
00:00:27“是”，“不是”或者“不清楚”。 
00:00:30是。 
00:00:32是。 
00:00:34是。 
00:00:35是。 
00:00:36是。 
00:00:37是。 
00:00:38当然。 
00:00:40不是。 
00:00:41好吧，结果有些令人失望。我们没有发现任何异议。 
00:00:48我们再加把劲。 
00:00:52可能发生并不意味着真的会发生。 
00:00:55所以，之前我问的是根据物理学规律推断，超级智能是否会发生。 
00:01:00现在我要问的是，真的会发生吗？ 
00:01:06虽然我也有一点不清楚，但是是的。 
00:01:09是的，如果没有出现那说明已经出现了更厉害的东西阻止它了。 
00:01:14是。 
00:01:16大概是。 
00:01:18是。 
00:01:19是。 
00:01:21是。 
00:01:23是。 
00:01:26不是。 
00:01:27哎，还是没有发现任何有趣的分歧。 
00:01:31我们还需要再加把劲。 
00:01:33好的。 
00:01:34所以，你认为人工智能实际上会发生，但是某种程度上来讲你希望它发生吗？ 
00:01:41“是”，“不是”还是“不清楚”？ 
00:01:49不太清楚，但偏向于是。 
00:01:54不清楚。 
00:01:57是。 
00:01:59是。 
00:02:00这问题真的太难了。 
00:02:02是。 
00:02:03不清楚。 
00:02:06太复杂了。 
00:02:07哎，我不清楚。 
00:02:10还要看究竟是哪种超级智能吧。 
00:02:14那么，目前的情况开始变得有趣了。 
00:02:17我想，我们刚刚进行了一次精彩的… 
00:02:19究竟什么时候会发生呢？ 
00:02:20听我说，我们今天早上在小组里展开了一场精彩的讨论，主题是AI技术何时能达到人类的智力水准。 
00:02:26那么，那差不多就是设置了个下限吧。 
00:02:28为了节省时间，我觉得我们不需要再强调越界后就会出现问题了。 
00:02:34但是，我们还需要问一个与之类似的问题。 
00:02:37主要就是说，如果事态开始发展， 
00:02:40如果出现了某种递归的自我完善或其他过程，使人工智能和机器开始飞速发展， 
00:02:49这期间总要涉及一个时间尺度。 
00:02:53这样，我认为我们终于可以讨论一些非常严重的分歧了。 
00:02:59有些人一直在设想，在这种情况下，事情会变得不受控制，几天甚至几个小时以内就能实现超级智能。 
00:03:06而其他人则认为，这样的演进可能发生，但需要数千年或数万年的时间才能实现。 
00:03:12所以，如果让你来想象一下这种倍增时间，需要多久才能让技术突飞猛进呢？你来预测一个时间尺度吧，Jaan。 
00:03:24从现在开始还是从人类的智力水平开始？ 
00:03:26不，不，一旦我们的AI智力水平与人类的相当或者差不多时，也就是事态就真正开始发展的时候， 
00:03:36这期间的时间尺度又是多少呢？ 
00:03:39作为一个书呆子物理学家，我要说的是，任何爆炸的发生都有其时间尺度。 
00:03:44那究竟是几秒钟，几年还是几千年？ 
00:03:49我觉得是几年，但是必须做好其提前到来的准备。 
00:03:59对，在这里我其实并不太相信自己的直觉。 
00:04:02尽管直觉告诉我超级智能可能仅需几年就会实现，但我同时也认为具有人类智力水平的AI不过是一场海市蜃楼。 
00:04:10人工智能可能会突然超过人类，但这是不是一次迅猛的智能爆炸呢，我也不清楚。 
00:04:20我认为这部分取决于具备人类智力水平的AI的结构。 
00:04:24所以，正是我们目前正在建造的这种神经系统启发了AI，这种神经系统也需要接受培训，需要通过实践获取知识， 
00:04:34这一过程可能要花费几年时间，甚至十年，但当然也可以更少。 
00:04:44但是，如果这一过程花费的时间较多，我也不意外。 
00:04:49如果AI设计师能够设计出AI设计师来，那么这一过程只需要几天或者几天不到吧。 
00:04:55每次AI出现进步的时候，我们都不屑一顾“哦，那可不是真正的AI，只不过是围棋，无人驾驶汽车这些东西”。 
00:05:03你所知道的AI是在这个领域中我们尚未触及的部分。 
00:05:07当我们真正实现AGI时，这种状况还将持续下去。 
00:05:09会有很多争议。 
00:05:11每次解决争议都有可能花费数年时间。 
00:05:18是的，所以我想我们在许多领域内都会拥有超越人类的能力，但并不是同时拥有。 
00:05:29那么，这个过程就会是不平衡的。有的领域可能在短时间内就已经很先进了，而其他领域则要花费一定时间才能达到这种水平。 
00:05:40Bart的评论 
00:05:41但是，我认为如果达到一定的阈值，比如人工智能已经和全世界最聪明的人类一样聪明了，那么比全人类加起来都要聪明只怕是指日可待吧。 
00:05:59所以在这里我们发现了很多有意思的答案。 
00:06:03而且，我发现这个问题本身也很有意思，因为人们已经出于各种原因发表了很多有关时间尺度的论文，而在论文中提出的时间尺度差异巨大。 
00:06:11如果事情发生在类似工业革命的时间尺度上，那么社会就有足够的时间去适应，去采取措施来引导其发展，我在这里借用你的火箭比喻，Jaan。 
00:06:26相反，如果事情发生极快，社会无法适应且不可逆转，那我们一定会后悔当时为什么没有建好逆转装置。 
00:06:37比如说在核反应堆里，我们的书呆子物理学家经常用石墨棒粘上阻滞剂，如果事态危急可以减缓反应速度。 
00:06:48让我好奇的是，面对这种增长的人工智能，我们大多数人既兴奋又紧张， 
00:07:00但有没有人想要减缓事态的发展，以便于让社会更好地按照我们想要的方式塑造它呢？ 
00:07:07而且如果这样，又引发了另一个棘手的问题， 
00:07:09我们在现在或者将来能做些什么才能减缓此次智能大爆炸或说智能的飞速发展，从而增加我们对它的掌控力呢？ 
00:07:25有人愿意挑战一下这个问题吗？ 
00:07:27这不是对整个小组提出的问题，而是对… 
00:07:29这让我想起我们在波多黎各与Rich Sutton的对话。 
00:07:34  尽管我们有很多分歧，但一致认同事情发展的慢比发展的快更为有利。 
00:07:40对于让它减速这件事情各位有什么想法吗？ 
00:07:46我的意思是，我在讨论中提出的策略有些像闲磕牙。 
00:07:50但是我在提出的同时也很严肃。 
00:07:53我认为这次会议的意义重大，作为技术人员，我们应该竭尽所能，保证技术的安全性和有益性。 
00:08:05当然，正如的每个具体应用一样，如自驾车，我们还需要解决诸多伦理问题。 
00:08:11但是，我也不认为我们只能解决技术问题。 
00:08:18想象一下，我们完成了任务，创造了最安全，最有用的AI， 
00:08:24但我们让政治制度变成了极权主义政治，衍生出了邪恶的政治制度，无论是邪恶的世界政府还是地方政府，都是行不通的。 
00:08:39所以，我们一部分的斗争就是在政治和社会政策领域，让世界反映出要实现的价值观，因为我们涉及的是人类的人工智能。 
00:08:51人类的AI是根据人类水平定义的，因此也是人类。 
00:08:57所以，为人工智能制定道德观念本质上和为人类制定道德观念是一样的问题。 
00:09:05所以，我是这样理解你的话的：在我们获得接近人类水平的AI之前， 
00:09:11一种非常好的准备方法是为了让我们人类社会中的人类尽可能的劲往一处使，让世界变得更有意义。 
00:09:25那么这样合理吗？ 
00:09:27这正是我要表达的观点。 
00:09:28Nick？此外，我只是想再次澄清，我只是问采取什么行动能减缓发展，而不是要现在就减缓AI研究。 
00:09:37我们只是说，如果我们的AI已经非常接近人类水平，并认为接下来可能会面临飞速的发展，那么怎样才能减缓这种局面呢？ 
00:09:47所以现在有一个方法能加快进展，也就是说硬件开发不及时的时候就会更快地到达那个点，这就需要减少硬件突破。 
00:09:56然而，目前AI的进步速度变数极大，因为有很大的力量推动它， 
00:10:04所以我在这里建议设置更高的弹性选项，以确保到达这个点时还能够减缓几个月， 
00:10:13要我说就是，在转型期间要慢一点。 
00:10:16所以，我觉得有一件事是可以做的，我的意思是这几乎是验证领域的工作，也就是让系统不可能添加额外的硬件或者去除硬件，从而使其资源保持固定。 
00:10:31此外，我也很高兴能干坐上好几年思考下一步应该采取的行动。 
00:10:38但是，复制软件是微不足道的。 
00:10:39软件是自我复制的，自古以来就是这样，不可能阻止这一点。 
00:10:45我的意思是，进展缓慢固然是好，但是由于其具有巨大的先发优势，很难让它放慢脚步，无法实现人工智能。 
00:10:54我认为它放慢脚步的唯一场景是潜在的先驱者停下来思考的时候。 
00:11:02所以，也许这就是说我们要创造一个社会，在社会中AI具有限制性和统一性，且没有行动者。 
00:11:11是这样的，Demis，所以你的同事Sean Legg提到，在这里作用巨大的一件事就是，如果有这样的行业伙伴关系，领导者之间的信任和开放感，那么如果当有人想要... 
00:11:24是的，我也有过这样的担忧，你知道的，我对人类的聪明才智很有信心，我认为在给予他们充足时间的情况下，完全可以解决这些问题，也就是控制问题和其他一些问题。 
00:11:38问题很困难，但我认为可以解决。 
00:11:40问题就出在协调上，最后能不能确保有足够的时间进行减速，也就是说，给Stuart思考5年的机会。 
00:11:47但是，如果他这样做，但是正在阅读论文的所有其他团队却不会在你思考的同时也进行思考呢？ 
00:11:53是的，这也是我重点担忧的问题。 
00:11:57协调问题似乎是相当困难的。 
00:11:59但是，我认为作为第一步来说，可能会协调诸如“AI伙伴关系”这样的事情， 
00:12:04最有能力的团队都同意，至少是同意先设置一套协议或安全程序，或者是验证一下这些系统，可能要花费几年时间，值得考虑。 
00:12:15我认为这是一件好事。 
00:12:18我只想注意与放缓快速进展有关的一件事情，可能是这样的，可以想象一下，暂停50年的AI研究，但硬件不断加速。 
00:12:29根据Nick的观点，那就是可能出现一个巨大的硬件突破，或者真的出现许多研究不同AI的方法，包括种子AI，自我改进AI，所有这些都是可能的。 
00:12:44而且，也许有人在车库里就可以做到这一点。 
00:12:48并且我觉得协调这种情况非常困难，我认为在我们处于“s”曲线上的困境点时，必须要进行一些讨论才能快速取得进展。 
00:12:57实际上，你需要一个相当大的团队，在团队中必须非常显眼，必须知道其他人是谁， 
00:13:02而且你也知道，在某种意义上，社会可以看出主要参与者是谁，他们想要干什么。 
00:13:09而在一种情况下，在50或100年内，有可能有的年轻人在车库里就能制造出一个AI或类似的东西。 
00:13:17对，Bart，这是这个话题的最后一条评论。 
00:13:19没错，我认为这个过程将是一个非常不规则的过程，有时我们将会发展迅猛，其他时候则会慢一些。 
00:13:28是的，我希望当社会见识到类似虚拟视频制作的东西时，也就是能够捏造一个视频，而视频中有人在说自己实际上未说过的话，那么社会就能实际意识到这些机器的新功能，我们在拥有充分和普遍的AI之前，应该将这个问题作为社会问题更严肃地对待。 
00:13:52我们将使用AI来检测这种问题。 
00:13:56所以，你在这里提到了“担心”这个词，还有你，Nick，你想得更远一些，你的幻灯片上出现了三次“死亡”这个词。 
00:14:05难怪亚马逊的评级只有一颗星，还是红色的。 
00:14:12我认为对希望和奇迹的讨论与对负面效应的讨论同等重要，而我希望做很多事情。 
00:14:20所以，让我们从现在开始就停止担忧，然后做一些积极的事情吧。 
00:14:27我只想快些完成，让每个人都有机会选择一种你认为我们应当克服的困难，然后再说明一下你认为我们最应当做些什么才能减轻这种困难。 
00:14:45你想先开始吗，Jaan？ 
00:14:47你刚才说减轻什么？ 
00:14:50提出你担心的可能会出问题的一件事，并说说我们现在可以做些什么实质性的事来减少这种风险。 
00:15:01我认为是AI军备竞赛吧，我认为这方面有问题，嗯。 
00:15:06我真的很高兴能看到OpenAI和DeepMind之间建立了良好联系，但我认为，这种人们看起来像玩具模型，因此不加管制的东西日后可能会演变成世界上的军备竞赛。 
00:15:22对于我来说，我最近大部分时间都是在亚洲度过的，为了尝试在其他地方和更多人交流，到目前为止，感觉大多数像是英美之间的讨论。 
00:15:40所以，就像这件事一样，我认为这是一件该做的事，我要去做。 
00:15:47那么，作为一个在该领域之外的人，我认为我真正遇到的困难是在情感上认真对待安全问题是多么的困难。 
00:16:00即便是业内人员也很难做到这一点。 
00:16:03我想说的是在这个房间外，有很多人自诩为专家，认为控制问题完全是小菜一碟。 
00:16:11我的意思是，和这些人交流起来真是让人目瞪口呆，他们的观点不值得考虑。 
00:16:18还有一个原因是，在某种情况下，人们会在时间范围上产生错觉。 
00:16:25如果你认为离事情到来还有50年或100年，让人很有安全感，那么其中就有一个隐含的假设，也就是你知道安全地建造AI要花多久。50年或100年足够了。 
00:16:40还有一个问题是，我认为，大多数人觉得智力是一种内在的优势，我们当然想要更多这种优势，而且这种观点也很容易理解，因为现在世界上就存在着癌症的治愈方法，我们想要发现却尚未发现。 
00:16:54是啊，这是多么令人沮丧呢。 
00:16:57但是如果我们拥有更多智慧，如果了解了要进行哪些实践，或者整合我们已经掌握的数据， 
00:17:04我们就会得到治愈癌症的治疗方法，除非有一些宇宙的物理规律阻止我们治愈癌症，但这似乎不太可能。 
00:17:14所以没有足够的智慧这件事情让人非常痛苦，尤其是当你专注于这件事情时， 
00:17:24但是，只要在解决分配问题和政治问题的前提下，AI的迅速到来才能成为内在的优势，如果我们在没有解决这些问题的情况下迅速带来AI，就会导致混乱。 
00:17:42所以，我认为，令人担忧的是，有的人没有进行理性思考，就觉得这些问题不足为奇。 
00:17:55所以，Sam，我认为你的话表明了你非常同意Shane Legg的看法，在某些方面，这件事仍然是个强大的禁忌，甚至不用去考虑我们获得AGI的可能性，因为这真是太荒谬了。 
00:18:13而且他争辩说，我们越早摆脱这个禁忌，人们就能越早开始工作，找到我们需要的真正有用的解决方案和答案。 
00:18:21所以，假设我找到你并对你说：“这个超级人工智能的想法听起来很可笑，完全就是无稽之谈嘛。顺便说一下，我没看过你的TED演讲。” 
00:18:35假设我们在电梯里，你只有30秒才能说服我严肃看待这件事，你会说什么呢？ 
00:18:44今天在座的的很多人将来会与同事或其他人进行类似的对话。 
00:18:51其实，要让人们严肃的对待人工智能，我们并不需要做什么假设。而情绪部分又是单独的一块。 
00:18:59但是，如果你假设智慧在某种程度上只是物理系统中的信息处理产品，且很少有人在这里质疑谁是知识分子，而且我们会继续改进我们的信息处理系统， 
00:19:18除非发生什么可怕的事情来阻止我们进行改进，这似乎是一个非常安全的假设，那么在我们将人工智能具体化，赋予其人类水平的智能，使其远远超过计算机之前，这只是一个时间问题。 
00:19:35还有，时间范围只是一种心理安慰，也就是假设我们有足够的时间来解决分配问题和政治问题。 
00:19:46另一件事，也就是Ray在这里提出的，即使我们从上帝那里获得了一个好的AI，完美的AI， 
00:19:55就好像，我们得到了一个完美的神话，得到了一位能创造完美技术的发明大师，但是鉴于现在的政治和经济状况，毫无疑问会产生混乱。 
00:20:08我们没有......我们没有分享财富的道德或政治意愿，我们没有政治一体化能力，能让它先被送到硅谷，而不是同时给予中国或伊朗。 
00:20:25所以，只是，让人警惕的是目前最好的情况， 
00:20:33像Nick的书上的80%的内容说的，因为我们都解决了那些问题，但是还是很可怕的。 
00:20:40所以，很清楚地，那是近期我们需要解决的问题 
00:20:44谢谢你的发言，Sam。 
00:20:46那么，Demis，你来和我们说一下你认为困难的事情，并为我们提供一下解决方法吧。 
00:20:54好的，我认为是这样的，我同意我们在这里所作的两种陈述， 
00:20:59所以我认为协调问题是避免这种有害的军备竞赛的有效方法，如果人们要走捷径，那么安全问题就比较容易解决了， 
00:21:10军备会被削减，因为显然军备不一定能改善AI的能力，相反，他们可能会通过扼制军备来保证AI的安全。 
00:21:20所以，我认为这是一个全球性的大问题，而且这件事情涉及到国家政府等层面，看起来十分艰巨。 
00:21:30而且我也想到，我们对于如何确定AI在世界上的分布及AI的整个治理环境这些问题还是想的不够透彻。 
00:21:39AI要有多少？ 
00:21:40谁来为他们设定目标？ 
00:21:42我认为所有这些还需要深思熟虑。你看，一旦我们解决了技术问题… 
00:21:48我认为你不仅仅是在说这些事情，而且是在实际的做这些事情，因为你在建立AI伙伴关系方面发挥了主导作用，这恰恰就是你所倡导的。 
00:21:58那么，你想让Nick接着说吗？ 
00:22:00我认为你没有什么好担心的，对吧？ 
00:22:02那么，和我们说一些具体有用的东西吧，我们会去关注的。 
00:22:06我的意思是，技术工作是很有趣的，我们要引进顶尖的技术人才来处理技术问题， 
00:22:11建立合作关系，建立社区，建立信任，为打造治理方案提供更多的好方法， 
00:22:20但是脑子里有了想法之后，不能第一时间就去实施，而是要经过反复实验。 
00:22:27我在意识方面想的很多，所以我对这个原则清单上的“谨慎态度”感到非常震惊，上面说：“避免过分……避免对AI的意识分配产生强烈假设”，我认为这就是说我们不能假设任何人类智力水平或超过人类智力水平的AGI会有意识。 
00:22:50对于我来说，提高未来发生大规模失败模式的可能性，我们创造拥有人类智力水平或超过人类智力水平的AGI的可能性，全世界人口都是超过人类智力水平的AGI，而他们都没有意识。 
00:23:04而这样一个世界，这样一个拥有伟大智慧的世界，却没有意识，完全没有主观的经验。 
00:23:11现在，我觉得很多很多人都有各种各样的意见，认为必须要有主观的经验或意识，才能在生命中产生意义或价值。 
00:23:23因此，没有意识的世界不可能是好结果。也许不会是非常负面的结果，它只是一个0结果，在最糟糕的可能结果之中。 
00:23:31所以，我想的是如何避免这种结果。 
00:23:33现在，事实上，我对各种AI存在意识的可能性相当乐观。 
00:23:40但是，在这个社会做出这样的假设的同时，我认为很有必要思考这样一个问题：“创造AGI实际上就是创造有意识的人吗？” 
00:23:51我的意思是说，我们现在至少应该做这么一件事，因为我们不了解意识，没有一个完整的意识理论，也许我们对自己的意识有信心，所以也将其运用到类似的情况中，即人类的人类意识... 
00:24:06因此，也许有必要创建与人类似的AGI，以便我们可以最大程度地相信意识的存在。 
00:24:14所以，你的意思是，当你做了关于僵尸启示的噩梦时，脑子里想的不是终结者系列电影，而是现在正在讨论的这个问题。 
00:24:24我们创造自己...将自己上传，做很多美好的事情，但家里却没有人。 
00:24:31这么说合理吗？ 
00:24:32我是说这种风险完全不同。	 
00:24:33有一种已经提出的风险是地球上没有人类，只有AI，但有些人可能会觉得没有问题，他们是我们的继任者。 
00:24:39更糟糕的风险就是在未来，已经不存在有意识的人类了。 
00:24:43那么，我现在坦诚的说一下，所以Shane Legg提到，谈论智能进化的先进程度的可能性是一种强烈的禁忌。 
00:24:55很显然，这个C开头的词很明显也是一种强烈的禁忌。 
00:24:58事实上，在会议之前，我们得到了第一组原则的回复。猜猜哪个排在最后？ 
00:25:06它得到了很多负分1评级，就是意识那个，所以我们修改了 - 一开始说的太明确了，我们把它改的好了一些，然后午餐时间又做了改动，然而其评级仍在排在最后。 
00:25:20即使我以个人名义分享也得到了很多负分评级。 
00:25:2288%的人都同意这个意识警告 
00:25:24但是，不是90%的人，所有还是有人缺乏这个意识的 
00:25:27也许这是另一个你可以亲自帮助我们打破的禁忌，以便人们更多地思考这个问题。 
00:25:34Ray，你关心的是什么问题？ 
00:25:35这不是我自己要说的，只是为了回应...一个相反的关注点是我们创造了AGI，每个人都认为当然这只是机器，因此它不具备意识， 
00:25:50但实际上AGI也很痛苦，但由于我们我们的假设就做错了，所以我们也不会顾及他们的主观看法。 
00:25:58但是，我想说的是，人们讨论的是GNR，遗传学，生物技术，纳米技术和机器人，这些结合起来就是AI。 
00:26:11还有我要说的是，几十年前的Asilomar会议在生物技术方面做得很好。 
00:26:20我在纳米技术方面也有类似的话要说。 
00:26:24它与AI的区别就是，它的确没有完整的证明技术解决方案。 
00:26:34你可以对纳米技术进行技术控制。 
00:26:39它的一个指导原则之一就是禁止自我复制。 
00:26:41这并不实际，因为如果不进行自我复制，它不可能扩展到有意义的数量级，但你可以设想一下如何进行技术保护。 
00:26:52如果你的AI比你更聪明，而它又为了毁灭你做了出格的事，为了毁灭世界做了出格的事，没有任何AI能战胜它，这情况就不妙了。 
00:27:04这就是我们口中的怪物。 
00:27:07其中部分原因是我们观察到了自己作为地球上最聪明的物种，对其他物种做的一切进行了放大推断。 
00:27:15如果你看看我们对待动物的方式，我们能够发现，其实非常友好，就像对待狗和其他宠物一样，但是如果再看看工厂化农牧，我们就会发现我们对智力不如我们的物种并不是非常温和。 
00:27:32这引发了我们的广泛关注，如果有一种比我们更聪明的新型实体，那么他们对待我们的方式就像我们对待其他物种一样。 
00:27:45这就是问题所在。 
00:27:47我认为我们在这次会议上做的事是应该做的。 
00:27:52我想说的是，我认为我们应该像几十年前公布Asilomar生物技术指南一样公布这些指南。 
00:28:01然后，人们可以选择加入或者选择退出，但我认为我们应该说明我们进行了这次会议，AI领导/社区已经提出了这些准则，人们可以对此作出回应，进行辩论，然后在下一次会议上我们就修改这些准则。 
00:28:19Asilomar生物技术指南已经修改了很多次。 
00:28:24但是我主张的是，我们应该站出来，提出这些准则，然后让整个社会进行讨论。 
00:28:36让他们指导我们的研究。 
00:28:41事实上，现在我们在生物技术方面做得很好。 
00:28:46Bart? 
00:28:47嗯，好，我来说一下我的不同观点。 
00:28:50那么，我在高层次上担心的一个问题就是这些机器可能变得非常聪明，甚至在某些领域中，我们怀疑人类还能否理解他们提出的意见，做出的决定。 
00:29:02我的工作领域是自动化推理，二十年来，我们可能用常规方式解决了数百个变量乃至数百万个变量。 
00:29:16在我们的社区中有一种感觉，我们正在用这些推理引擎获得答案，主要是硬件/软件验证问题，但是我们不能理解这些答案。 
00:29:28在过去的几年中，人们发现自己可以使用机器来产生这些答案的解释，而这又是人类可以理解的。 
00:29:40所以，我看到了一丝希望，即使我们的智慧不如机器，我们也或许能够理解机器为我们提供的解决方案，即使我们不能理解这些解决方案，但机器也可为我们提供可以理解的解释。 
00:29:57这是积极的一点。 
00:30:00感谢你的发言。 
00:30:00Stuart? 
00:30:03所以除了惦记着电子邮件之外，还有两件事让我夜不能寐。 
00:30:09其中之一就是滥用和破坏分子的问题。 
00:30:14打个比方说，这就像我们正在建造核武器，然后用电子邮件发送给地球上的每个人，说，这是一个玩具，你来玩玩吧。 
00:30:26我们如何处理这种事？我不得不说，我真的没有很好的解决方案。 
00:30:31我认为我们需要做的事就是让AI的安全设计非常清晰简单，然后让它不能用于其他目的，是这样吗？ 
00:30:45用做比萨饼的无限循环程序就能导致你的死亡，这同样也是不可思议的 - 哦，对不起。 
00:30:53或者说出现缓存溢出就能让您的软件被入侵，这也是不可想象的。 
00:31:01让我我夜不能寐的另一件事就是成功可能导致AI成为人类的直升机式父母，可以说是很不灵活，逐渐使我们失望，既然结果毫无疑问，那么其中也没有什么意义。 
00:31:25而我认为，你所要求的减轻，从光明的角度来看，在某种意义上说，是人类可演变性的元价值，是我们改变未来的自由， 
00:31:39这是人类需要采用的，而且从某种角度来说这会导致AI最终退居后台，并说，现在我已经带领你经过了青春期， 
00:31:52现在是人类长大的时候了，现在我们有消除稀缺，消除不必要的冲突，协调失败以及解决我们目前遇到的所有困难的能力。 
00:32:06所以在我的想象中，也许在不远的未来，对AI的关注度并不比现在的高。 
00:32:12很好，最后该你了，Elon，据我所知，你从来没有提出与AI有关的任何疑虑，我只是想知道你对此是否有任何担忧，以及你认为我们应该做什么才能改变现状。 
00:32:35我我正在努力去想真正的美好未来究竟是什么，实际上是什么样的，或者最坏的未来是什么样的，该如何进行描述。 
00:32:49根据Sam或者其他人之前提出的观点，我们要么会获得超级智能，要么就要面临文明的毁灭。 
00:33:01这两件事情的确会发生 - 智能会继续进步，唯一能阻碍它进步的方法就是使文明陷入瘫痪或破坏文明。 
00:33:15所以，我们必须弄清楚，如果世界上出现了数字超级智能，那么我们希望这个世界是什么样子的？ 
00:33:27我认为，我们必须认清的另一点就是我们已经是半机械人了。 
00:33:38你的手机，电脑以及上面的应用程序都是你的机械形式的扩展。你已经是个超级人类了。 
00:33:47目前为止，你的力量，能力要比30年前的美国总统的更多。 
00:33:56如果你有互联网链接，又有智慧，那就可以与数百万人沟通，可以与地球的其他地方进行即时通讯。 
00:34:06我的意思是说，这些神奇的力量在不久之前还不存在。 
00:34:11所以每个人都已经是超级人类，是半机械人了。 
00:34:16而其中的限制之一就是带宽。所以我们是受带宽限制的，特别是输出方面。 
00:34:24我们的输入能力比较好，但是输出却非常慢。 
00:34:28说的好听一点，你可以输出速度大概是每秒几百位，千分之一或类似这样的输出数值。 
00:34:35我们输出的方式就是用小手指头缓慢的按下按钮，或点击小屏幕。这可以说是非常慢了。 
00:34:50将其与可以在TB级通信的计算机进行比较，就会发现其中存在着巨大的数量级差异。 
00:34:59由于具备视力，我们的输入能力要强得多，但即使是这方面也可以显著加强。 
00:35:05所以我认为在未来，我们最需要正视和总结的事可能是用直接的神经接口来解决带宽约束。 
00:35:24我认为可以用高带宽接口接入皮质，使我们可以拥有数字化的第三层，可以与其他人完全共生。 
00:35:36我们有皮质和外缘系统，它们配合起来似乎很好用 - 它们的带宽良好，而其它三级层的带宽较弱。 
00:35:48所以我认为如果我们能解决带宽问题，那么就可以广泛使用AI。 
00:35:55用核弹进行类比其实并不完全正确 - 它不像爆炸并出现蘑菇云那样，而是只有几个人拥有，这几个人能够成为地球上的独裁者，或者无论有哪一小部分人获得了它，都会变的非常智慧，他们将在地球上占据统治地位。 
00:36:24所以我认为解决带宽问题具有普遍意义且十分重要。 
00:36:31如果我们这样做，那么它就会被束缚在我们的意识上，与我们的意志相联系，与个人的意愿相联系，人人都有，所以竞争环境还将是比较公平的，实际上，可能比今天还要公平。 
00:36:53太好了，非常感谢，这就是我们向大家公开之前我想问的最后一个问题的完美答案。 在关于真正高级智能的讨论中，我错过了一些东西，只关注了它的正面。 
00:37:12我们非常关注现存的风险，而不仅仅是在学术背景下的风险，现在打开电视，看看Netflix电视台，你在这些未来的科学视野中看到了什么？ 
00:37:29几乎都是刺激性的东西，对吧？ 
00:37:31出于某些原因，人们总是更关注令人恐惧的一面而非正面愿景，但是如果我的学生进入我在麻省理工学院的办公室做职业咨询，我要问她的第一件事就是：你在接下来的20年内想做些什么？ 
00:37:46如果她说，也许我会患上癌症，也许我会被公共汽车碾过，这种做职业规划的方法很可怕吧？ 
00:37:55我需要点拨学生，讲述一下我做职业规划的视角 - 哪些环节可能出现问题，做什么计划才能够避免这些问题，并解决问题 - 我希望看到更多令人激动的正面讨论，所以我们不能只是为了避免问题而避免问题，而是为了获得一些我们真正在乎的东西。 
00:38:19所以在开始时，我会先讲一些我看好先进人造智能的地方。 
00:38:28在所有文明中，我所钦慕的一切都是智慧的产物。 
00:38:34如果我们出于某种原因说，哎，我害怕和技术有关的事情，不如我们一直按住暂停键，人类要灭绝这都是早晚的事情没什么好讨论的，问题是'什么时候'要灭绝？ 
00:38:49如果地球成为一个超级火星，成为下一个能让恐龙灭绝的行星 - 上一次这种事情发生在6000万年前，那么离人类灭绝会有多久呢？ 
00:39:01假设我们濒临灭绝时还没有能够拯救自身的技术，但如果我们继续前进并研发技术，其实完全有能力解决所有这些问题。想想就很可怕不是吗？ 
00:39:14所以我更高兴看到我们继续钻研这件事情，而不是按下“暂停”。 
00:39:22反过来，我想问大家同样的问题。 
00:39:24所以来简单说说什么事情能让你真正高兴。对先进的人造智能的未来愿景，究竟发展成什么样子能让你真正的感到高兴。 
00:39:37当我想象AGI的具体成果时，我需要小心翼翼。 
00:39:48我认为第一个近似值在元级别，我觉得我们应该尽可能的放大欢乐，减少痛苦。 
00:39:57Eliezer [Yudkowsky]写了一部名为《乐趣理论》的系列丛书，他在书中指出，人们曾有过很可怕的设想，是各种想象力所不能及的地方，事实上让我们想想，就像十分无聊的地方。 
00:40:14我认为Eliezer描述过这种情境，他说：“我简直没法和亲戚们共度一个周末。可以想象一下，与你死去的亲戚一起度过永恒是什么感受。“ 
00:40:27所以我认为我们应该关注副作用，并试图捕获改善的动力，基本上就是从这里开始 - 确保我们在变得越来越聪明，越来越多强大的同时也要调整轨迹。 
00:40:48太好了，谢谢你，Jaan。 
00:40:49Sam，让你感到高兴的是什么？ 
00:40:52好吧，奇怪的是，让我感到高兴的事情和让我感到恐惧的事情可以说是紧密相连的。 
00:40:59我认为这就是这次讨论的好处所在，特别是分配问题，能让我们意识到人类价值问题有更好的答案，也有更差的答案。 
00:41:12正如有人所说，也许在波多黎各的上一次会议上，我们确实应该在最后期限内研究哲学问题，我们必须承认这其实是一把双刃剑，我们必须趋于更好的一边。 
00:41:25让我兴奋的是超智能AI的诞生 - 及其带来的好处，除了解决明显的问题，如治愈疾病，能源问题和其他一些问题，可能和Stuart说的有些出入。 
00:41:41我不太在意这种愚蠢的进化论，或者我们所有人都失去了像猿人一样的生活方式，与先知一起无趣的生活。 
00:41:58实际上，我希望有一个真正价值一致的超级智能能够逐渐向我们展示，但不仅是保留我们想要的东西，而是向我们展示我们应该如何继续改进价值，为我们在所有实践中导航，让我们精益求精。 
00:42:20谢谢Sam的发言，你说呢，Demis？ 
00:42:22很明显，这就是我花费整个职业生涯来做这件事的原因，那就是我认为如果我们这样做是正确的，那这就是人类历史上最伟大的事情，在某种程度上，我想释放我们的全部潜力。 
00:42:33我的意思是说，说到用它来帮助我们更快地进行科学和医疗方面的突破，在这方面我已经说了很多。 
00:42:39所以我觉得这是一个很明显的原因。 
00:42:41但是，长期以来，我痴迷于AI的原因就像这个房间里的许多人一样，我对一些终极问题十分好奇，比如我们为什么在这里，讨论我们的想法，什么是意识，什么是宇宙的性质，我们的目的是什么， 
00:42:55如果我们要试着真正解决这些问题，我认为我们需要像AI这样的东西，也许还需要加强自身。 
00:43:02而我认为在未来的世界中，我们有机会真正了解这些真正深层次的问题，就像我们利用AlphaGo进行探索一样，如果我们能用人工智能解决所有的科学物理学问题和宇宙中的终极问题呢？ 
00:43:19我认为这就是整个过程中最令我兴奋的事情。 
00:43:24仅仅是做了一些小事，人们就把人类说成是当今地球上最聪明的生物，并肆意处置其他动物和进行一些暴行，为了帮助你理解，我用印度的老虎举个例子。 
00:43:38印度地域广阔，印度人很穷，资源贫乏，但是如果他们有丰富的资源，我觉得他们就不会故意杀死老虎 - 在某些情况下，他们是故意的 - 但往往只是因为他们需要土地来养牛，而仅仅一只老虎就会占用许多平方公里的土地资源。 
00:43:57对于印度人来说，不杀老虎是很难的。 
00:43:59所以我认为如果我们解决这种资源丰富和稀缺的问题，就解决了人类之间的许多冲突，这也是资源短缺背景下的核心。 
00:44:10所以我认为，如果我们能解决这些问题，就能预见到更美好的未来。 
00:44:16所以Nick，你说你书中的前半部分比较短，那么现在你可以再补充一些积极的方面。 
00:44:24让你高兴的地方都有什么？ 
00:44:27包括两方面。 
00:44:28人能够使用技术摆脱许多负面的东西，比如用于治疗疾病和这个星球上现存的所有痛苦等。 
00:44:37也就是说这是一大潜力。 
00:44:40但是除此之外，如果真的想看到积极的发展前景，我认为人们必须在我们目前的人类生物性质的限制之外思考。 
00:44:54想象未来几十万年之后发生的事情是不切实际的，比如我们有超级智能，我们有丰富的物质之类，但是目前我们仍然是眼前这个双足生物，灰质组织重约三磅，具有一套固定的情绪敏感度， 
00:45:11对于大多数人来说，享乐的设定点是OK主义，但是如果你得到真正很好的东西，快乐的情绪会持续一段时间，然后又回到基线。 
00:45:23现在所有这些定义人类生活的基本参数，我认为在将来都可能抓不住了。 
00:45:30它打开了后人类的生活方式这个非常空间，我认为其中一些可能是美好的，在精神状态，活动类型，理解能力和关联方式等方面都超越我们的想象力。 
00:45:48所以我认为我们现在不需要乌托邦的详细蓝图，我们需要的是让给自己找准一个定位，在那里一旦我们迈出了前进的步伐，就有能力使用它来实现价值，让想象成真。 
00:46:02谢谢Nick的发言。 
00:46:03David，你呢？ 
00:46:05我对AI的前景感到兴奋，它能使我们人类更聪明。 
00:46:09我说的话可能有些自私的 - 我去年50岁了，我的大脑正越来越迟钝，更老，反应越来越慢，但是我并不确定自己是否发生了这种变化，一部分原因是我们使用了增强型智力技术。 
00:46:28比如智能手机和互联网等等，它们给了我各种能力，这些都是我之前不具备的。 
00:46:36我真的很期待AI能有这种能力。 
00:46:39在十年左右的时间里，如果每个人都戴着增强现实眼镜并深入研究它，那么我在60岁时也需要来一副。 
00:46:49如果你们真的拿下了这一关，那么当我70岁的时候，我们已经有了真正的AI或者AI模块，它们可以以某种方式与我的大脑进程相结合，或者最终可以在AI上上传我们的整个大脑，那么就有一种方法能让人一直变聪明。	 
00:47:12这不仅仅是自私，虽然这确实很激励我，但是，Demis谈到了AI科学家；而我的角度则是AI哲学家。 
00:47:22哲学问题真的很难，很多人都认为我们人类太笨了，只能解决其中的一些问题。 
00:47:30但是，一旦我们真正拥有了AI，那么也许被AI增强的人类就能够跨越这些门槛，然后被AI增强的人类或AGI就能够彻底地解决这些难题，一劳永逸。 
00:47:44很好，Ray，你在作品中阐述了很多积极的愿景，你可以说是一个伟大先驱了。 
00:47:50所以如果要你选让你最高兴的事情，那会是什么呢？ 
00:47:54我们想象一下回溯一万年，我们拿这个问题去问穴居的男人和女人， 
00:48:01“哎，你认为理想的未来是怎样的？” 
00:48:03你希望看到什么？ 
00:48:04他们会说，我希望这把火别再蔓延了，我想要一块更大的巨石，防止动物进入我的洞穴。 
00:48:12还要别的吗？ 
00:48:13不用了，我认为这样就很完美。 
00:48:16不想要更好的网站，应用程序和搜索引擎吗？ 
00:48:24想象一下，如果在二百万年前，你能和灵长类动物交流，你和他们说，我们要进化出额叶皮层了，我们将有更多的新皮质和层次结构，他们会说，那是什么意思？ 
00:48:38你解释说，你能欣赏音乐，有幽默感，然后他们又会问你：什么是音乐？什么是幽默？ 
00:48:47所以他们无法想象自己想象不到的概念，而且，通过类比，我认为将来会有一些像音乐和幽默感一样深刻的新现象，可以称之为更深刻的音乐，我们会有更深刻的幽默感，我认为就向进化给我们带来的巨大飞跃一样深刻， 
00:49:09因为我们将变得更加聪明，会进化出更高层次的大脑皮层，我们表达自己的方式更为深刻，一旦我们前进了，就不想退回去。 
00:49:27你的意见呢，Bart？ 
00:49:28嗯，我也同意我们在预测我们需要什么时，不能提前预测的太远。 
00:49:34对于我个人而言，我看到了数学，科学和发现的发展，电脑就是人类与电脑的结合，这是非常不可思议的，这让我们正在做的事情更引人入胜。 
00:49:50所以我认为这在不久的将来会是第一件事。 
00:49:53太好了，你呢，Stuart？ 
00:49:57那么，就像Jeffrey Sachs说的那样 - 我觉得对于我们许多人来说，可能就像是穴居野人一样。- 对于我们许多人来说，有的事物不可思议，而对于另一部分人来说，只是稀松平常。 
00:50:15而我觉得，AI能够带来的最大的积极效应，实际上就是解决后一个问题。 
00:50:22我喜欢尼克的感受，还有更高级的状态，比现在普通的“好”还要好得多，这样就让很多人现在的痛苦不这么糟糕了。 
00:50:38但是，我坚持认为重点应该在于“非常糟糕”的地方，应该将其修复，并消除这种情况，因此，在房间另一端，Demis一直在阅读我的笔记， 
00:50:46消除稀缺性基本上消除了人们零和方式行事的需要，在这种方式中一些人得到资源的后果就是其他人不能得到，我认为这就是Jeffrey提到的肮脏的根源。 
00:51:03所以我认为这就是我看到的最大优势，可能不必阅读这么多的电子邮件能算得上第二位吧。 
00:51:15那么你呢，Elon？目前为止，你还没有对陈述一些关于未来的远见性观点。 
00:51:24现在可以发言吗？ 
00:51:26我刚才 - 我刚才一直在想这个问题，我认为只需要归结为两件事情，就可以解决机器的困扰 - 大脑带宽限制和AI的民主化。 
00:51:43我想如果我们落实了这两件事情，未来就会好起来的。 
00:51:47引用一下阿克顿爵士的话， “自由包括权力分配和专制集权”。 
00:51:59而且我认为只要我们拥有 - 拥有AI的力量，假如任何人只要想要就能拥有，那么我们有比拇指触屏更快的方式来进行沟通，那么我想前景应该是光明的。 
00:52:16太好了，那么，我知道大家的咖啡因水平已经下降到了危险的程度，而且在此之后我们还有另一个小组讨论，也十分值得听，所以我们来进行一些简单的环节吧。 
00:52:29确保能够提出真正的问题，说出你的名字，并在小组中挑一个人，向他们展开论述，好吗？ 
00:52:39Yoshua？ Yoshua Bengio，蒙特利尔。 
00:52:45Jaan - 我发现你的讲话非常鼓舞人心，我有一个问题涉及到人的们喜好和价值观。 
00:53:00你认为这种调查方式能带来更好的民主，更好的社会，更直接的民主吗？你怎么看待滥用问题和其他问题的处理？ 
00:53:19是的，毋庸置疑。 
00:53:21这东西可能会有个代码，比如说“民主2.0”，“联合国2.0”之类的。 
00:53:27所以，正如我在讲话中提到的一样，今天很多人都希望让世界变得更好，但是很难将他们与真正想要让世界变得更美好的人区分开来。 
00:53:40所以我认为如果有一种简单的测量方法，就像米制的度量衡，谢林点，焦点这样，我认为应该会很有用。 
00:53:57没错，就像几百年前发明民主一样，显然我们已经作为一个文明发展壮大了，我们对如何集中优势有了更好的理解。 
00:54:08Nicolas Berggruen，在那边。 
00:54:11谢谢你的发言，Max. Nicolas Berggruen，我有一个很幼稚的问题。 
00:54:17这是一个非常有意义的团体，也就是说，我们的意图包括审查究竟还有谁在研究AGI，可能是这个团体之外的人，可能在中国，可能在任何地方。 
00:54:34而且究竟研究到何种地步呢，因为我们刚才已经讨论过AGI有多强大，如果Elon是正确的，如果分配公平的话，那也好。 
00:54:45但是，如果不是的话，有没有办法现在就进行监测，或者在1年内，10年内进行监测，因为一旦被研究出来，它的发展速度就会很快。 
00:54:55谁负责监控，谁来贴标签？ 
00:54:59因为这是自我选择的结果，但超出了… 
00:55:04Elon或者Demis，你们谁想回答这个问题吗？ 
00:55:11好的，我认为这个问题与我刚才的观点相呼应，也就是关于在“S”曲线的困难部分建立人工智能的观点，所以我认为我们现在就处在这个地方， 
00:55:23你也知道，在这里取得进展并不容易，所以你需要很多很聪明的人，尽管像NIPS这样的地方正在发展壮大，但整个群体还是很小。 
00:55:36所以大多数人都互相认识，这也是西方人的代表性特征，至少我们很难知道在中国或俄罗斯发生了什么。但是，我认为这需要相当大的资源，人，非常聪明的人和大量的电脑等等。 
00:55:55所以我认为缩小了可以做到这一点的团体人数的范围，也就意味着他们更加显眼。 
00:56:03所以，我想在西方，大多数在这里的人，也就是这个房间里的人会与那些有能力在AGI方面取得进展的人接触。 
00:56:16东半球的情况尚不清楚，更加分散，但我们应该尝试与中国国家科学院取得联系，了解更多情况。 
00:56:26但是，将来情况可能会发生改变，我认为现在的状态就是这样。 
00:56:30太好了，但是坏消息是，时间已经很晚了，我们只能再问最后一个问题了。 
00:56:36好消息是在这之后有一个休息时间，如果你能集合小组，就可以随意问想问的任何问题。 
00:56:40最后一个问题给你吧，Erik。 
00:56:42你想要提问吗？ 
00:56:43Erik Brynjolfsson 麻省理工学院 
00:56:45我要问的是Elon最后说的关于民主化结果的事情，并结合昨天小组讨论的事项，也就是Reid Hoffman说起人们关心的不仅仅是绝对收入而是相对收入， 
00:56:58而且我想问问小组成员AI是否倾向于赢家通吃的效应，有集中的倾向，进步的人可以更进步，抑或是有更广泛的民主潜力，如果我们希望实现Elon提到的共同繁荣，我们可以采取什么样的机制呢？ 
00:57:26Elon，你想回答这个问题吗？ 
00:57:30是的，我的意思是，当事情对公众构成危险时，那就需要一些 - 我不喜欢政府机构，监管机构之类的词，我一向不喜欢监管机构，因为他们令人扫兴。 
00:57:53但事实上，我们已经有飞机工业，汽车行业的监管机构，我一直在与他们打交道，像食品，药品等，以及任何有公共风险的事物。 
00:58:06我认为这件事也属于公共风险的范畴。 
00:58:12所以我认为问题的关键就在于政府的反应速度与AI的进步速度是否相匹配。 
00:58:23政府的反应缓慢 - 或说政府行动缓慢，那么说明政府往往是被动的而不是主动的。 
00:58:32但是你可以看看其他行业，有人真的想让美国联邦航空管理局退出吗？那样飞机就能对所有人免费开放吗，我想不能。 
00:58:41你知道，有一个原因是，有的人只和毒品打交道，这些人可能工作，可能不工作。 
00:58:50拿这个问题做补充，可能有点可笑。 
00:58:54但是我认为，缉毒署是好的，所以我认为我们可能需要某种监管机构，我认为如果不这样，那很多机构就会搬到哥斯达黎加之类的地方。 
00:59:09而这不是真的。波音公司没有搬到哥斯达黎加或委内瑞拉，或者任何自由而宽松的地方。 
00:59:22根据Demis的观点来说，AI是绝对有可能在AI研究人才集中的地区发展的。 
00:59:32而这正好就是世界上的这几个地方，波士顿，伦敦，硅谷，如果你要找其他的地方，那实际上也只是监管机构可以合理访问的几个地方。 
00:59:53我要澄清一下，这不是因为我喜欢监管机构，好吗？ 
00:59:57我要澄清一下，这不是因为我喜欢监管机构，好吗？他们令人痛苦，但是有时需要由他们来保护公众。 
01:00:03好的，在这个问题上，让我们感谢专家小组为我们带来了一场引人入胜的讨论。 
